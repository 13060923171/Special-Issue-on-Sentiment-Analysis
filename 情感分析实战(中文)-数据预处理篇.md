# 情感分析实战(中文)-数据预处理与情感分类模块

**背景：该专栏的目的是将自己做了N个情感分析的毕业设计的一个总结版，不仅自己可以在这次总结中，把自己过往的一些经验进行归纳，梳理，巩固自己的知识从而进一步提升，而帮助各大广大学子们，在碰到情感分析的毕业设计时，提供一个好的处理思路，让广大学子们能顺利毕业**

## 废话不多说：该专栏主要有2大主题：

## 1、中文-情感分析实战：

a) [数据获取篇]()

b) [数据预处理篇-情感分类篇]()

c) [无监督学习机器学习聚类篇]()

d) [LDA主题分析篇]()

e) [社交网络篇]()

## 2、英文-情感分析实战:

a) [数据预处理篇]()

b) [数据预处理篇-情感分类篇]()

c) [无监督学习机器学习聚类篇]()

d) [LDA主题分析篇]()

e) [社交网络篇]()

------



------

一般来说，中文文本处理主要包括以下几个步骤：

1. 分词：将一段中文文本切割成一个个单独的词语，便于后续处理。
2. 词性标注：给每个切分出来的词语打上词性标记，例如动词、名词、形容词等，以便更好地理解句子的结构和含义。
3. 命名实体识别：识别文本中出现的人名、地名、组织机构名等实体，并进行分类和命名。
4. 情感分析：分析一段文本传达的情感色彩，例如正面、负面、中性等，以便做出针对性的决策。
5. 文本分类：将文本归入一个预定义的类别中，例如新闻报道、评论、广告等。

以上几个步骤通常是中文文本处理的主要流程，不同的任务可能会侧重其中的某些步骤。



这里采用的是京东评论的文本数据来进行预处理，数据如图所示：

分别对应的是用户id，评分，内容，创建时间，分类

![image-20230606144636525](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230606144636525.png)

我们主要的数据列是content，也是从这里进行数据预处理，一共分5步走：

- 分词，采用jieba库来进行分词
- 进行词性处理，把名词和形容词保存下来，其他的词就去掉，不考虑在本次的文本分析中，这个是不同情况需要不同对待，因为这次主要是做情感分析，故此这样
- 采用停用词，去掉一下不必要的词汇
- 进行机械压缩处理，主要是为了删除"哈哈哈哈哈哈哈哈哈哈哈哈哈"，这样的词汇,留下哈哈
- 判断是否为中文，不是中文的，一律去掉

代码如下：

```python
import pandas as pd
import numpy as np
import re
import jieba
import jieba.posseg as pseg

#去掉重复行以及空值
df = pd.read_csv('data.csv')
df.drop_duplicates(subset=['content'], inplace=True)

#导入停用词列表
stop_words = []
with open("stopwords_cn.txt", 'r', encoding='utf-8') as f:
    lines = f.readlines()
    for line in lines:
        stop_words.append(line.strip())
#判断是否为中文
def is_all_chinese(strs):
    for _char in strs:
        if not '\u4e00' <= _char <= '\u9fa5':
            return False
    return True

# 定义机械压缩函数
def yasuo(st):
    for i in range(1, int(len(st) / 2) + 1):
        for j in range(len(st)):
            if st[j:j + i] == st[j + i:j + 2 * i]:
                k = j + i
                while st[k:k + i] == st[k + i:k + 2 * i] and k < len(st):
                    k = k + i
                st = st[:j] + st[k:]
    return st
  
def get_cut_words(content_series):
    # 对文本进行分词和词性标注
    words = pseg.cut(content_series)
    # 保存名词和形容词的列表
    nouns_and_adjs = []
    # 逐一检查每个词语的词性，并将名词和形容词保存到列表中
    for word, flag in words:
        if flag.startswith('n') or flag.startswith('a'):
            if word not in stop_words and len(word) >= 2 and is_all_chinese(word) == True:
                # 如果是名词或形容词，就将其保存到列表中
                nouns_and_adjs.append(word)
    if len(nouns_and_adjs) != 0:
        return ' '.join(nouns_and_adjs)
    else:
        return np.NAN
        
df['content'] = df['content'].apply(yasuo)
df['分词'] = df['content'].apply(get_cut_words)
new_df = df.dropna(subset=['content'],axis=0)
new_df.to_csv('new_data.csv', encoding='utf-8-sig', index=False)
        

```





接着在上面数据处理完成之后，我们开始情感分类

这里采用的是transformers来进行情感分类任务：https://github.com/huggingface/transformers

![image-20230606164226570](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230606164226570.png)

安装方法如下：

![image-20230606164307799](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230606164307799.png)



使用代码：

```Python
classifier = pipeline('sentiment-analysis')
label_list = []
score_list = []
for d in new_df['content']:
    class1 = classifier(d)
    label = class1[0]['label']
    score = class1[0]['score']
    if score <= 0.55:
        label = 'NEUTRAL'
        label_list.append(label)
    else:
        label = label
        label_list.append(label)
    score_list.append(score)

new_df['情感类型'] = label_list
new_df['情感得分'] = score_list

new_df.to_csv('new_data.csv', encoding='utf-8-sig', index=False)
```



最后效果图呈现：

![image-20230606170738076](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230606170738076.png)