# 情感分析实战(英文)-数据预处理与情感分类模块

**背景：该专栏的目的是将自己做了N个情感分析的毕业设计的一个总结版，不仅自己可以在这次总结中，把自己过往的一些经验进行归纳，梳理，巩固自己的知识从而进一步提升，而帮助各大广大学子们，在碰到情感分析的毕业设计时，提供一个好的处理思路，让广大学子们能顺利毕业**

## 情感分析实战：

a) [数据获取篇](https://blog.csdn.net/zyh960/article/details/131172565?spm=1001.2014.3001.5501)

b) [数据预处理篇-情感分类篇(中文版)](https://blog.csdn.net/zyh960/article/details/131172656?spm=1001.2014.3001.5501)

c) [数据预处理篇-情感分类篇(英文版)](https://blog.csdn.net/zyh960/article/details/131172163?spm=1001.2014.3001.5501)

d) [无监督学习机器学习聚类篇](https://blog.csdn.net/zyh960/article/details/131172511?spm=1001.2014.3001.5501)

e) [LDA主题分析篇](https://blog.csdn.net/zyh960/article/details/131172253?spm=1001.2014.3001.5501)

f) [共现语义网络](https://blog.csdn.net/zyh960/article/details/131172433?spm=1001.2014.3001.5501)

------



------

如果清洗的是英文文本数据，并在此基础上进行自然语言处理（NLP）的分析，以下是一般的英文文本数据清洗步骤：

1. 去除特殊字符：删除数据中不需要的符号和字符，包括标点、符号和HTML标签等，以确保文本数据为纯文本。
2. 去除停用词：去除不必要的单词和停用词，例如“a”、“the”，这些词对文本分析没有实际的贡献。
3. 标准化单词：将文本数据中的词汇规范化，例如将不同的大小写、时态或形式的同一单词标准化为统一的形式。
4. 词干提取和词形还原：对文本数据中的词汇进行词干提取（stemming）和词形还原（lemmatization）操作，以减少文字内容中的冗余，并使其更加高效。
5. 实体识别：如果文本涉及具体实体或命名实体，需要进行实体识别操作。
6. 拼写检查：进行拼写检查操作，以识别和修正数据中存在的拼写错误。
7. 情感分析：运用NLP工具，进行情感分析，识别文本中的正面、负面或中性情感，并为其打分。

以上是一般的英文文本数据清洗步骤，具体的清洗过程取决于数据集本身的特点和实际需求。需要注意的是，文本分析是一个复杂的任务，并且只有在为文本建立正确的清洗步骤并清洗好数据后才能得到准确的分析结果。





这里采用的是Twitter的数据内容：

![image-20230612155747204](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230612155747204.png)



首先去标点符号：

```Python 
#去掉标点符号，以及机械压缩
def preprocess_word(word):
    # Remove punctuation
    word = word.strip('\'"?!,.():;')
    # Convert more than 2 letter repetitions to 2 letter
    # funnnnny --> funny
    word = re.sub(r'(.)\1+', r'\1\1', word)
    # Remove - & '
    word = re.sub(r'(-|\')', '', word)
    return word

#再次去掉标点符号
def gettext(x):
    import string
    punc = string.punctuation
    for ch in punc:
        txt = str(x).replace(ch,"")
    return txt
```



接着去停用词：

```Python 
stop_words = []
with open('常用英文停用词(NLP处理英文必备)stopwords.txt','r',encoding='utf-8')as f:
    lines = f.readlines()
    for line in lines:
        stop_words.append(line.strip().replace("'",""))
```



再替换表情包，在数据中，会有很多表情包，有时候这些表情包容易影响文本的判断，只好去掉

```python
#替换表情包
def handle_emojis(tweet):
    # Smile -- :), : ), :-), (:, ( :, (-:, :')
    tweet = re.sub(r'(:\s?\)|:-\)|\(\s?:|\(-:|:\'\))', ' ', tweet)
    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D
    tweet = re.sub(r'(:\s?D|:-D|x-?D|X-?D)', ' ', tweet)
    # Love -- <3, :*
    tweet = re.sub(r'(<3|:\*)', ' ', tweet)
    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;
    tweet = re.sub(r'(;-?\)|;-?D|\(-?;)', ' ', tweet)
    # Sad -- :-(, : (, :(, ):, )-:
    tweet = re.sub(r'(:\s?\(|:-\(|\)\s?:|\)-:)', ' ', tweet)
    # Cry -- :,(, :'(, :"(
    tweet = re.sub(r'(:,\(|:\'\(|:"\()', ' ', tweet)
    # Angry -- >:(, >:-(, :'(
    tweet = re.sub(r'(>:\(|>:-\(|:\'\()', ' ', tweet)
    # Surprised -- :O, :o, :-O, :-o, :0, 8-0
    tweet = re.sub(r'(:\s?[oO]|:-[oO]|:0|8-0)', ' ', tweet)
    # Confused -- :/, :\, :-/, :-\
    tweet = re.sub(r'(:\\|:/|:-\\\\|:-/)', ' ', tweet)
    # Embarrassed -- :$, :-$
    tweet = re.sub(r'(:\\$|:-\\$)', ' ', tweet)
    # Other emoticons
    # This regex matching method is taken from Twitter NLP utils:
    # https://github.com/aritter/twitter_nlp/blob/master/python/emoticons.py
    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\)|\(|D|P)', tweet)
    for emoticon in emoticons:
        tweet = tweet.replace(emoticon, " ")
    return tweet

```



接着先进行标准化单词 再对词干提取和词形还原：

```python
def clean_text(text):
    # Replaces URLs with the word URL
    text = re.sub(r'((www\.[\S]+)|(https?://[\S]+))', ' URL ', text)
    # Replace @username with the word USER_MENTION
    text = re.sub(r'@[\S]+', ' __USER_MENTION__ ', text)
    # Replace #hashtag with the word HASHTAG
    text = re.sub(r'#(\S+)', ' __HASHTAG__ ', text)
    # Remove RT (retweet)
    text = re.sub(r'\brt\b', ' ', text)
    # Replace 2+ dots with space
    text = re.sub(r'\.{2,}', ' ', text)
    # Strip space, " and ' from text
    text = text.strip(' "\'')
    # Handle emojis
    text = handle_emojis(text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    # Remove numbers
    text = re.sub(r'\d+', ' ', text)
    # Remove punctuation
    text = re.sub(r'[^A-Za-z0-9\s]+', ' ', text)
    # Lowercase and split into words
    words = text.lower().split()
    words = [w for w in words if w not in stop_words]
    words = [lemmatizer.lemmatize(w) for w in words]
    words = [preprocess_word(w) for w in words]
    if len(words) != 0:
        return ' '.join(words)
    else:
        return np.NAN
```

在做完上面的步骤之后，我们就可以进行情感分析了

这里采用的是transformers来进行情感分类任务：https://github.com/huggingface/transformers

![image-20230606164226570](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230606164226570.png)

安装方法如下：

![image-20230606164307799](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230606164307799.png)

使用代码如下：

```python
classifier = pipeline('sentiment-analysis')
label_list = []
score_list = []
for d in new_df['clearn_comment']:
    class1 = classifier(d)
    label = class1[0]['label']
    score = class1[0]['score']
    if score <= 0.55:
        label = 'NEUTRAL'
        label_list.append(label)
    else:
        label = label
        label_list.append(label)
    score_list.append(score)
```





做完上面的步骤之后，我们查看一下我们的总的结果如何：

![image-20230612161248638](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230612161248638.png)

效果还是很不错的



整体代码如下：

```Python
import pandas as pd
import re
import numpy as np
from tqdm import tqdm
import time
from nltk.stem import WordNetLemmatizer
from transformers import pipeline

df1 = pd.read_csv('./data/en_data1.csv')
df2 = pd.read_csv('./data/en_data1.csv')
df = pd.concat([df1,df2],axis=0)
lemmatizer = WordNetLemmatizer()


stop_words = []
with open('常用英文停用词(NLP处理英文必备)stopwords.txt','r',encoding='utf-8')as f:
    lines = f.readlines()
    for line in lines:
        stop_words.append(line.strip().replace("'",""))

#去掉标点符号，以及机械压缩
def preprocess_word(word):
    # Remove punctuation
    word = word.strip('\'"?!,.():;')
    # Convert more than 2 letter repetitions to 2 letter
    # funnnnny --> funny
    word = re.sub(r'(.)\1+', r'\1\1', word)
    # Remove - & '
    word = re.sub(r'(-|\')', '', word)
    return word

#再次去掉标点符号
def gettext(x):
    import string
    punc = string.punctuation
    for ch in punc:
        txt = str(x).replace(ch,"")
    return txt


#替换表情包
def handle_emojis(tweet):
    # Smile -- :), : ), :-), (:, ( :, (-:, :')
    tweet = re.sub(r'(:\s?\)|:-\)|\(\s?:|\(-:|:\'\))', ' ', tweet)
    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D
    tweet = re.sub(r'(:\s?D|:-D|x-?D|X-?D)', ' ', tweet)
    # Love -- <3, :*
    tweet = re.sub(r'(<3|:\*)', ' ', tweet)
    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;
    tweet = re.sub(r'(;-?\)|;-?D|\(-?;)', ' ', tweet)
    # Sad -- :-(, : (, :(, ):, )-:
    tweet = re.sub(r'(:\s?\(|:-\(|\)\s?:|\)-:)', ' ', tweet)
    # Cry -- :,(, :'(, :"(
    tweet = re.sub(r'(:,\(|:\'\(|:"\()', ' ', tweet)
    # Angry -- >:(, >:-(, :'(
    tweet = re.sub(r'(>:\(|>:-\(|:\'\()', ' ', tweet)
    # Surprised -- :O, :o, :-O, :-o, :0, 8-0
    tweet = re.sub(r'(:\s?[oO]|:-[oO]|:0|8-0)', ' ', tweet)
    # Confused -- :/, :\, :-/, :-\
    tweet = re.sub(r'(:\\|:/|:-\\\\|:-/)', ' ', tweet)
    # Embarrassed -- :$, :-$
    tweet = re.sub(r'(:\\$|:-\\$)', ' ', tweet)
    # Other emoticons
    # This regex matching method is taken from Twitter NLP utils:
    # https://github.com/aritter/twitter_nlp/blob/master/python/emoticons.py
    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\)|\(|D|P)', tweet)
    for emoticon in emoticons:
        tweet = tweet.replace(emoticon, " ")
    return tweet


def clean_text(text):
    # Replaces URLs with the word URL
    text = re.sub(r'((www\.[\S]+)|(https?://[\S]+))', ' URL ', text)
    # Replace @username with the word USER_MENTION
    text = re.sub(r'@[\S]+', ' __USER_MENTION__ ', text)
    # Replace #hashtag with the word HASHTAG
    text = re.sub(r'#(\S+)', ' __HASHTAG__ ', text)
    # Remove RT (retweet)
    text = re.sub(r'\brt\b', ' ', text)
    # Replace 2+ dots with space
    text = re.sub(r'\.{2,}', ' ', text)
    # Strip space, " and ' from text
    text = text.strip(' "\'')
    # Handle emojis
    text = handle_emojis(text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)
    # Remove numbers
    text = re.sub(r'\d+', ' ', text)
    # Remove punctuation
    text = re.sub(r'[^A-Za-z0-9\s]+', ' ', text)
    # Lowercase and split into words
    words = text.lower().split()
    words = [w for w in words if w not in stop_words]
    words = [lemmatizer.lemmatize(w) for w in words]
    words = [preprocess_word(w) for w in words]
    if len(words) != 0:
        return ' '.join(words)
    else:
        return np.NAN



df['clearn_comment'] = df['comment'].apply(gettext)
df['clearn_comment'] = df['clearn_comment'].apply(preprocess_word)
df['clearn_comment'] = df['clearn_comment'].apply(clean_text)
new_df = df.dropna(subset=['clearn_comment'],axis=0)

classifier = pipeline('sentiment-analysis')
label_list = []
score_list = []
for d in new_df['clearn_comment']:
    class1 = classifier(d)
    label = class1[0]['label']
    score = class1[0]['score']
    if score <= 0.55:
        label = 'NEUTRAL'
        label_list.append(label)
    else:
        label = label
        label_list.append(label)
    score_list.append(score)

new_df['情感类型'] = label_list
new_df['情感得分'] = score_list

new_df.to_csv('./data/en_new_data.csv', encoding='utf-8-sig', index=False)

```



[整体项目地址](https://github.com/13060923171/Special-Issue-on-Sentiment-Analysis)

（小声bb：如果这个项目对你有用，不妨给我一个免费的小星星，非常感谢！！！）
