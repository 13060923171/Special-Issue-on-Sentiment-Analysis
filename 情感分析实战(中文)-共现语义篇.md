# 情感分析实战(中文)-共现语义网络分析

**背景：该专栏的目的是将自己做了N个情感分析的毕业设计的一个总结版，不仅自己可以在这次总结中，把自己过往的一些经验进行归纳，梳理，巩固自己的知识从而进一步提升，而帮助各大广大学子们，在碰到情感分析的毕业设计时，提供一个好的处理思路，让广大学子们能顺利毕业**

## 废话不多说：该专栏主要有2大主题：

## 1、中文-情感分析实战：

a) [数据获取篇](https://blog.csdn.net/zyh960/article/details/131083616?spm=1001.2014.3001.5501)

b) [数据预处理篇-情感分类篇](https://blog.csdn.net/zyh960/article/details/131083683?spm=1001.2014.3001.5501)

d) [无监督学习机器学习聚类篇](https://blog.csdn.net/zyh960/article/details/131090242?spm=1001.2014.3001.5501)

e) [LDA主题分析篇](https://blog.csdn.net/zyh960/article/details/131092799?spm=1001.2014.3001.5501)

f) [共现语义网络](https://blog.csdn.net/zyh960/article/details/131095544?spm=1001.2014.3001.5502)

## 2、英文-情感分析实战:

a) [数据预处理篇]()

b) [数据预处理篇-情感分类篇]()

c) [无监督学习机器学习聚类篇]()

d) [LDA主题分析篇]()

e) [共现语义网络]()

------



------



在中文NLP文本分析中，共现语义网络是一种常用的文本分析框架，它的目的是在文本中寻找单词的相互关联性，生成一个共同出现单词的图形网络。其主要意义在以下方面：

1. 挖掘主题：通过构建共现语义网络，可以快速识别文本中的核心主题和话题，以帮助人们更好地理解文本的内容和主旨。共现语义网络可以清晰地呈现不同单词之间的关系，形成有意义的语义网状图，直观地展示多重主题和热点。

2. 识别实体和关系：在共现语义网络中，词语之间的边界和联系通常表示实体和关系。通过这些边界和联系，可以快速识别并提取出文本中的实体和关系。这对于处理命名实体识别、关系抽取和实体链接等NLP任务非常有用。

3. 支持信息检索：共现语义网络还可以作为信息检索的一种方式，特别是针对一些领域特定的文本数据，通过构建网络，可以更加精细地检索和匹配相关的文本。与基于关键词匹配的搜索相比，基于共现语义网络的检索方法更加优化和智能。

4. 优化机器学习模型：共现语义网络可以对于机器学习任务如分类、聚类等，作为重要的特征和数据来源。将文本数据表示成共现语义网络中的节点和边，会优化模型的特征表示和表达能力，提高模型的预测和泛化能力。

总之，共现语义网络是一种帮助人们理解中文文本和数据的重要工具。在不同的场景下，使用共现语义网络进行分析可以获得丰富的信息和洞见，支持各种文本分析任务。



接下来具体实现思路如下：

1、先运行jieba库进行分词处理，由于分词前面已经处理好了，这里就不过多描述

```Python 
def fenci():
    f = open('./共现语义/fenci.txt', 'w', encoding='utf-8-sig')
    for line in df['分词']:
        line = line.strip('\n')
        # 停用词过滤
        seg_list = jieba.cut(line, cut_all=False)
        cut_words = (" ".join(seg_list))

        # 计算关键词
        all_words = cut_words.split()
        c = Counter()
        for x in all_words:
            if len(x) >= 2 and x != '\r\n' and x != '\n':
                if is_all_chinese(x) == True and x not in stop_words:
                    c[x] += 1
        output = ""
        for (k, v) in c.most_common(30):
            output += k + " "

        f.write(output + "\n")
    else:
        f.close()
```



而后进行共线矩阵计算：

共现矩阵是一种用来表示单词之间共现关系的矩阵，常用于文本分析和自然语言处理中。在计算共现矩阵时，通常先定义一个大小为N的词表，将文本中出现的所有单词映射到词表中的位置，然后基于这个词表计算共现矩阵。共现矩阵的计算思路如下：

1. 定义词表和矩阵：首先定义一个大小为N的词表，其中每个单词对应一个位置，然后定义一个大小为N×N的共现矩阵M，其中第i行第j列的元素表示第i个单词与第j个单词共同出现的次数。

2. 遍历文本：然后遍历文本中的每一个子串，并将其中的单词对应的位置的元素加1。具体来说，可以使用滑动窗口的方法，在文本中按照窗口大小（通常为2或3）间隔遍历子串，并对子串中出现的单词在共现矩阵中对应位置的元素加1。

3. 根据共现原则计算矩阵：最后，可以基于共现原则对矩阵进行归一化处理，计算两个单词之间的共现概率。通常使用点互信息（Pointwise Mutual Information, PMI）或者余弦相似度（Cosine Similarity）等方法来计算两个单词的共现概率。

共现矩阵的计算可以用如下的矩阵公式表示：

$$
M 
i,j
​
 = 

D
∑
d=1
​
 [t 
i
​
 ,t 
j
​
 ] 
d
​
$$
其中$[t_i,t_j]_d$表示文档d中单词$t_i$和$t_j$的共现情况，$M_{i,j}$表示构成的共现矩阵中第i行第j列元素的值。

具体代码实现步骤：

```python
f = open("./共现语义/fenci.txt", encoding='utf-8')
line = f.readline()
while line:
    line = line.replace("\n", "")  # 过滤换行
    line = line.strip('\n')  # 过滤换行
    nums = line.split(' ')
    # 循环遍历关键词所在位置 设置word_vector计数
    i = 0
    j = 0
    while i < len(nums):  # ABCD共现 AB AC AD BC BD CD加1
        j = i + 1
        w1 = nums[i]  # 第一个单词
        while j < len(nums):
            w2 = nums[j]  # 第二个单词
            # 从word数组中找到单词对应的下标
            k = 0
            n1 = 0
            while k < len(word):
                if w1 == word[k]:
                    n1 = k
                    break
                k = k + 1
            # 寻找第二个关键字位置
            k = 0
            n2 = 0
            while k < len(word):
                if w2 == word[k]:
                    n2 = k
                    break
                k = k + 1

            # 重点: 词频矩阵赋值 只计算上三角
            if n1 <= n2:
                word_vector[n1][n2] = word_vector[n1][n2] + 1
            else:
                word_vector[n2][n1] = word_vector[n2][n1] + 1
            j = j + 1
        i = i + 1
    # 读取新内容
    line = f.readline()
f.close()

words = codecs.open("./共现语义/word_node_1.txt", "w", "utf-8")
i = 0
while i < len(word):
    student1 = word[i]
    j = i + 1
    while j < len(word):
        student2 = word[j]
        if word_vector[i][j]>0:
            words.write(student1 + " " + student2 + " "
                        + str(word_vector[i][j]) + "\r\n")
        j = j + 1
    i = i + 1
words.close()
```



接着我们根据上面生成好的文件，对文件进行重新处理，保存为CSV文件，并且删除一些无效数据，空数据，并且让权重从大到小进行排序

接着使用networkx库对数据进行可视化操作

```python
""" 第四步:图形生成 """
with open('./共现语义/word_node_1.txt','r',encoding='utf-8')as f:
    content = f.readlines()
list_word1 = []
list_word2 = []
list_weight = []
for i in content:
    c = i.strip('\n').split(" ")
    list_word1.append(c[0])
    list_word2.append(c[1])
    list_weight.append(c[2])

df = pd.DataFrame()
df['word1'] = list_word1
df['word2'] = list_word2
df['weight'] = list_weight
df['weight'] = df['weight'].astype(int)
df = df.sort_values(by=['weight'],ascending=False)
df = df.dropna(how='any',axis=1)
new_df = df.iloc[0:150]

A = []
B = []
for w1,w2 in tqdm(zip(new_df['word1'],new_df['word2'])):
    if w1 != "" and w2 != "":
        A.append(w1)
        B.append(w2)
elem_dic = tuple(zip(A,B))
print(len(elem_dic))
#创建一个空的无向图。即创建了一个称为G的图对象，用于保存文本数据的节点和边信息。
G = nx.Graph()
#向图G中添加节点和边。这里的list(elem_dic)表示将elem_dic字典中的元素列表作为图的边。其中elem_dic字典中存储着文本数据的节点和边信息。
G.add_edges_from(list(elem_dic))
#设置图像中使用中文字体，以避免出现显示中文乱码的情况。这里将字体设置为SimHei，使用sans-serif字体族。
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['font.family'] = 'sans-serif'
#设置图像的大小，其中figsize参数设置图像的宽度和高度。
plt.figure(figsize=(20, 14))
#确定节点布局。这里使用了一种称为spring layout的布局算法，相当于在二维空间中对节点进行排列。iterations参数指定了进行节点排列的迭代次数。
pos=nx.spring_layout(G,iterations=10)
#绘制节点。其中alpha参数设置节点的透明度，node_size参数设置节点的大小。
nx.draw_networkx_nodes(G, pos, alpha=0.7,node_size=1600)
#绘制边。其中width参数设置边的宽度，alpha参数设置边的透明度，edge_color参数设置边的颜色。
nx.draw_networkx_edges(G,pos,width=0.5,alpha=0.8,edge_color='g')
#添加标签。其中font_family参数指定图像中使用sans-serif字体族，alpha参数设置节点标签的透明度，font_size参数设置归纳节点标签的字体大小。
nx.draw_networkx_labels(G, pos, font_family='sans-serif', alpha=1,font_size='24')
plt.title("共现语义")
plt.savefig('./共现语义/共现语义.png')
plt.show()
```



最后的效果呈现：

![共现语义](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E5%85%B1%E7%8E%B0%E8%AF%AD%E4%B9%89.png)

整体代码如下：

```python
import codecs
import networkx as nx
import matplotlib.pyplot as plt
import csv
import pandas as pd
import nltk
from collections import Counter
from scipy.sparse import coo_matrix
from tqdm import tqdm
import numpy as np
import re
import jieba


def main():
    df = pd.read_csv('./data/new_data.csv')

    def fenci():
        f = open('./共现语义/fenci.txt', 'w', encoding='utf-8-sig')
        for line in df['分词']:
            line = line.strip('\n')
            # 停用词过滤
            seg_list = jieba.cut(line, cut_all=False)
            cut_words = (" ".join(seg_list))

            # 计算关键词
            all_words = cut_words.split()
            c = Counter()
            for x in all_words:
                if len(x) >= 2 and x != '\r\n' and x != '\n':
                    if is_all_chinese(x) == True and x not in stop_words:
                        c[x] += 1
            output = ""
            for (k, v) in c.most_common(30):
                output += k + " "

            f.write(output + "\n")
        else:
            f.close()

    fenci()

    word = [] #记录关键词
    f = open("./共现语义/fenci.txt", encoding='utf-8')
    line = f.readline()
    while line:
        #print line
        line = line.replace("\n", "") #过滤换行
        line = line.strip('\n')
        for n in line.split(' '):
            #print n
            if n not in word:
                word.append(n)
        line = f.readline()
    f.close()

    word_vector = coo_matrix((len(word),len(word)), dtype=np.int8).toarray()
    print(word_vector.shape)

    f = open("./共现语义/fenci.txt", encoding='utf-8')
    line = f.readline()
    while line:
        line = line.replace("\n", "")  # 过滤换行
        line = line.strip('\n')  # 过滤换行
        nums = line.split(' ')
        # 循环遍历关键词所在位置 设置word_vector计数
        i = 0
        j = 0
        while i < len(nums):  # ABCD共现 AB AC AD BC BD CD加1
            j = i + 1
            w1 = nums[i]  # 第一个单词
            while j < len(nums):
                w2 = nums[j]  # 第二个单词
                # 从word数组中找到单词对应的下标
                k = 0
                n1 = 0
                while k < len(word):
                    if w1 == word[k]:
                        n1 = k
                        break
                    k = k + 1
                # 寻找第二个关键字位置
                k = 0
                n2 = 0
                while k < len(word):
                    if w2 == word[k]:
                        n2 = k
                        break
                    k = k + 1

                # 重点: 词频矩阵赋值 只计算上三角
                if n1 <= n2:
                    word_vector[n1][n2] = word_vector[n1][n2] + 1
                else:
                    word_vector[n2][n1] = word_vector[n2][n1] + 1
                j = j + 1
            i = i + 1
        # 读取新内容
        line = f.readline()
    f.close()

    words = codecs.open("./共现语义/word_node_1.txt", "w", "utf-8")
    i = 0
    while i < len(word):
        student1 = word[i]
        j = i + 1
        while j < len(word):
            student2 = word[j]
            if word_vector[i][j]>0:
                words.write(student1 + " " + student2 + " "
                    + str(word_vector[i][j]) + "\r\n")
            j = j + 1
        i = i + 1
    words.close()

    """ 第四步:图形生成 """
    with open('./共现语义/word_node_1.txt','r',encoding='utf-8')as f:
        content = f.readlines()
    list_word1 = []
    list_word2 = []
    list_weight = []
    for i in content:
        c = i.strip('\n').split(" ")
        list_word1.append(c[0])
        list_word2.append(c[1])
        list_weight.append(c[2])

    df = pd.DataFrame()
    df['word1'] = list_word1
    df['word2'] = list_word2
    df['weight'] = list_weight
    df['weight'] = df['weight'].astype(int)
    df = df.sort_values(by=['weight'],ascending=False)
    df = df.dropna(how='any',axis=1)
    new_df = df.iloc[0:150]

    A = []
    B = []
    for w1,w2 in tqdm(zip(new_df['word1'],new_df['word2'])):
        if w1 != "" and w2 != "":
            A.append(w1)
            B.append(w2)
    elem_dic = tuple(zip(A,B))
    print(len(elem_dic))
    #创建一个空的无向图。即创建了一个称为G的图对象，用于保存文本数据的节点和边信息。
    G = nx.Graph()
    #向图G中添加节点和边。这里的list(elem_dic)表示将elem_dic字典中的元素列表作为图的边。其中elem_dic字典中存储着文本数据的节点和边信息。
    G.add_edges_from(list(elem_dic))
    #设置图像中使用中文字体，以避免出现显示中文乱码的情况。这里将字体设置为SimHei，使用sans-serif字体族。
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['font.family'] = 'sans-serif'
    #设置图像的大小，其中figsize参数设置图像的宽度和高度。
    plt.figure(figsize=(20, 14))
    #确定节点布局。这里使用了一种称为spring layout的布局算法，相当于在二维空间中对节点进行排列。iterations参数指定了进行节点排列的迭代次数。
    pos=nx.spring_layout(G,iterations=10)
    #绘制节点。其中alpha参数设置节点的透明度，node_size参数设置节点的大小。
    nx.draw_networkx_nodes(G, pos, alpha=0.7,node_size=1600)
    #绘制边。其中width参数设置边的宽度，alpha参数设置边的透明度，edge_color参数设置边的颜色。
    nx.draw_networkx_edges(G,pos,width=0.5,alpha=0.8,edge_color='g')
    #添加标签。其中font_family参数指定图像中使用sans-serif字体族，alpha参数设置节点标签的透明度，font_size参数设置归纳节点标签的字体大小。
    nx.draw_networkx_labels(G, pos, font_family='sans-serif', alpha=1,font_size='24')
    plt.title("共现语义")
    plt.savefig('./共现语义/共现语义.png')
    plt.show()


if __name__ == '__main__':
    main()
```

