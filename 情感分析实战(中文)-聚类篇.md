# 情感分析实战(中文)-无监督学习 k-means聚类分析

**背景：该专栏的目的是将自己做了N个情感分析的毕业设计的一个总结版，不仅自己可以在这次总结中，把自己过往的一些经验进行归纳，梳理，巩固自己的知识从而进一步提升，而帮助各大广大学子们，在碰到情感分析的毕业设计时，提供一个好的处理思路，让广大学子们能顺利毕业**

## 情感分析实战：

a) [数据获取篇](https://blog.csdn.net/zyh960/article/details/131172565?spm=1001.2014.3001.5501)

b) [数据预处理篇-情感分类篇(中文版)](https://blog.csdn.net/zyh960/article/details/131172656?spm=1001.2014.3001.5501)

c) [数据预处理篇-情感分类篇(英文版)](https://blog.csdn.net/zyh960/article/details/131172163?spm=1001.2014.3001.5501)

d) [无监督学习机器学习聚类篇](https://blog.csdn.net/zyh960/article/details/131172511?spm=1001.2014.3001.5501)

e) [LDA主题分析篇](https://blog.csdn.net/zyh960/article/details/131172253?spm=1001.2014.3001.5501)

f) [共现语义网络](https://blog.csdn.net/zyh960/article/details/131172433?spm=1001.2014.3001.5501)

------



------

**K-means聚类是数据挖掘分析领域中被广泛应用的一种聚类算法。它的背景是为了通过对数据集进行聚类来发现其中的结构和模式，从而对数据进行有效性分析和处理。**

具体来说，K-means聚类通过将数据分为K个簇，每个簇包含距离最近的数据样本。它基于数据样本间的距离或相似性，将它们归到同一簇中，从而实现聚类分析的目的。K-means聚类的核心思想是找到最小化其内部方差的聚类中心，从而获得最优的聚类结果。

通过对中文数据集进行K-means聚类，能够帮助我们对大量文本数据进行自动分类、提取主题和识别特点，从而使得我们能够更加有效地获取和分析数据。例如在文本挖掘中，我们可以对大量的新闻文本进行K-means聚类，将相似的新闻聚合到一起，以便更好地进行新闻分析和资讯推荐等任务。在商业应用方面，我们可以通过对顾客的购物数据进行聚类，以了解各个消费群体的特点和需求，从而实现精确营销和产品优化等目的。



因此为了将文本进行自动分类，所以我们采用聚类的模式对文本进行自动分类

首先我们将进行特征选择：

**特征选择：在这里我们直接使用了原始的分词结果，可能存在某些无意义的词语，可以使用一些特征选择方法来去除这些对聚类没有贡献的特征。**

代码如下：

```Python
# 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频
vectorizer = CountVectorizer()

# 该类会统计每个词语的tf-idf权值
transformer = TfidfTransformer()

# 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
# 获取词袋模型中的所有词语
word = vectorizer.get_feature_names_out()

# 将tf-idf矩阵抽取出来 元素w[i][j]表示j词在i类文本中的tf-idf权重
weight = tfidf.toarray()
```



接着我们通过**轮廓系数评估来确定最优的聚类数量**:

轮廓系数是一种用于评估聚类质量的指标，用于衡量聚类后的聚类效果。其具体定义为：

对于一个样本，设该样本所处的聚类为C，同一聚类内其他样本的平均距离为`a(i)`，该样本点与最近的簇中的其他样本的平均距离为`b(i)`，则该样本的轮廓系数为:

$$
s(i)= 
b(i)−a(i)/
max{a(i),b(i)}
$$
对于一个聚类，其轮廓系数的平均值被称为该聚类的轮廓系数（silhouette score）。聚类的轮廓系数越高，表示该聚类内部实例越密集，不同聚类之间的距离也越大。

在实践中，轮廓系数通常用于评估聚类算法参数的优劣性，例如聚类数目K。通过计算不同参数的轮廓系数，选择轮廓系数最高的参数组合，可以获得更优的聚类效果。

因此，轮廓系数是一种非常重要的聚类评估指标，可以帮助我们评估聚类效果、选择聚类数量和参数、优化聚类算法等。



代码如下：

```Python
silhouette_scores = []
best_k = None
# 设置聚类数量K的范围
range_n_clusters = range(2, 11)
# 计算每个K值对应的轮廓系数
for n_clusters in range_n_clusters:
	kmeans = KMeans(n_clusters=n_clusters)
    labels = kmeans.fit_predict(weight)
    score = silhouette_score(weight, labels)
    silhouette_scores.append(score)

    if best_k is None or score > silhouette_scores[best_k - 2]:
		best_k = n_clusters

# Print the best K value and its corresponding silhouette score
print(f"Best K value: {best_k}")
print(f"Silhouette score for best K value: {silhouette_scores[best_k - 2]}")
# 绘制轮廓系数图
plt.plot(range_n_clusters, silhouette_scores, 'bo-', alpha=0.8)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.title('Silhouette score for K-means clustering')
plt.savefig('轮廓系数图.png')
plt.show()

n_clusters = best_k
```



效果如同所示：

![1231](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0%E5%9B%BE.png)



而后根据所获得的最优参数去进行重新聚类，得出相应的结果，**并且使用pca进行降维处理**



PCA(Principal Component Analysis)是一种经典的数据降维方法。在聚类分析中，采用PCA降维的目的是为了减少特征数量，消除不必要的数据维度，保留数据的主要信息，并更好地展示数据的结构。PCA可以将原始的高维数据转换为低维度的数据，这对于聚类算法中高维数据的分析和处理有很大的帮助。

PCA的降维过程是将样本数据投影到一个低维空间，从而保留最大化的数据方差。该过程可以被看作是捕获数据中的主成分，将原始数据的高维特征向量转换为一些低维度的特征向量，并通过这些主成分来表示数据的结构。

这样做的意义在于：

1. 减少运算量：原始数据维度高时，聚类算法的计算量也随之增大。采用PCA降维可以明显减少高维数据量，从而减小算法的运算量和时间成本。

2. 删除不重要的数据：通过PCA降维可以处理不必要的数据维度，只保留有用的特征信息。该过程将复杂的高维数据空间简化为更易理解和可视化的低维空间，提高聚类效率。

3. 可视化：PCA降维使得高维数据更具有可视化性，聚类后的数据可以更容易被可视化的展示出来，增加分析的可理解性和可解释性。

4. 增强聚类效果：对于数据中存在噪音和冗余变量的情况，通过PCA降维可以将数据中不含信息的特征减少，并更好地展示聚类所要求分组的内在结构。这将有助于更快速和准确地对数据进行聚类分析，并提高聚类效果。

PCA降维是一种常用的高维数据处理方法，对于聚类分析中的高维数据的处理，采用PCA降维可以更好地结构化和简化高维度的数据信息，获得更高效、更准确的聚类结果。

具体代码如下：

```Python 
 # 使用PCA对特征进行降维
pca = decomposition.PCA(n_components=2)
pca.fit(weight)
pca_vectors = pca.transform(weight)

clf = KMeans(n_clusters=n_clusters, random_state=111)
pre = clf.fit_predict(pca_vectors)

x = [n[0] for n in pca_vectors]
y = [n[1] for n in pca_vectors]
plt.figure(figsize=(12, 9), dpi=300)
plt.rcParams['font.sans-serif'] = ['SimHei']  # 支持中文
plt.rcParams['axes.unicode_minus'] = False
plt.scatter(x, y, c=pre, s=100)
plt.title("聚类分析图")
plt.savefig('聚类分析图.png')
plt.show()

df['聚类结果'] = list(pre)
df.to_csv('聚类结果.csv', encoding="utf-8-sig",index=False)
```

降维过后：

可以看出，每个颜色划分的都很明确，没有出现重叠，说明这次聚类的效果很棒

![聚类分析图](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E5%9B%BE.png)

![image-20230607160601216](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20230607160601216.png)

聚类的结果呈现就如图所示，一般如果没有class那一列的时候，需要对该文本进行划分的时候，通过聚类就可以帮助我们有效的对文本进行划分类别，接着根据这些类别进行下一步分析内容，通过这些类别划分，我们可以去分析某一类，情感占比如何，用户反馈如何，他们发帖的频率等等做到一些我们需要的分析内容





总体代码：

```python
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import re
import jieba
import numpy as np
from collections import Counter
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import PCA
from sklearn import decomposition
from sklearn import metrics
import os
from sklearn.metrics import silhouette_score


sns.set_style(style="whitegrid")


def kmeans():
    corpus = []
    df = pd.read_csv('new_data.csv',encoding='utf-8-sig')
    # 读取预料 一行预料为一个文档
    for d in df['分词']:
        corpus.append(d.strip('\n'))

    # 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频
    vectorizer = CountVectorizer()

    # 该类会统计每个词语的tf-idf权值
    transformer = TfidfTransformer()

    # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵
    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
    # 获取词袋模型中的所有词语
    word = vectorizer.get_feature_names_out()

    # 将tf-idf矩阵抽取出来 元素w[i][j]表示j词在i类文本中的tf-idf权重
    weight = tfidf.toarray()

    silhouette_scores = []
    best_k = None
    # 设置聚类数量K的范围
    range_n_clusters = range(2, 11)
    # 计算每个K值对应的轮廓系数
    for n_clusters in range_n_clusters:
        kmeans = KMeans(n_clusters=n_clusters)
        labels = kmeans.fit_predict(weight)
        score = silhouette_score(weight, labels)
        silhouette_scores.append(score)

        if best_k is None or score > silhouette_scores[best_k - 2]:
            best_k = n_clusters

    # Print the best K value and its corresponding silhouette score
    print(f"Best K value: {best_k}")
    print(f"Silhouette score for best K value: {silhouette_scores[best_k - 2]}")

    # 绘制轮廓系数图
    plt.plot(range_n_clusters, silhouette_scores, 'bo-', alpha=0.8)
    plt.xlabel('Number of clusters')
    plt.ylabel('Silhouette score')
    plt.title('Silhouette score for K-means clustering')
    plt.savefig('轮廓系数图.png')
    plt.show()

    # n_clusters = best_k
    n_clusters = 3
    # 打印特征向量文本内容
    print('Features length: ' + str(len(word)))
    # 第二步 聚类Kmeans
    print('Start Kmeans:')

    # 使用PCA对特征进行降维
    pca = decomposition.PCA(n_components=2)
    pca.fit(weight)
    pca_vectors = pca.transform(weight)

    clf = KMeans(n_clusters=n_clusters, random_state=111)
    pre = clf.fit_predict(pca_vectors)

    x = [n[0] for n in pca_vectors]
    y = [n[1] for n in pca_vectors]
    plt.figure(figsize=(12, 9), dpi=300)
    plt.rcParams['font.sans-serif'] = ['SimHei']  # 支持中文
    plt.rcParams['axes.unicode_minus'] = False
    plt.scatter(x, y, c=pre, s=100)
    plt.title("聚类分析图")
    plt.savefig('聚类分析图.png')
    plt.show()

    df['聚类结果'] = list(pre)
    df.to_csv('聚类结果.csv', encoding="utf-8-sig",index=False)


if __name__ == '__main__':
    kmeans()

```



[整体项目地址](https://github.com/13060923171/Special-Issue-on-Sentiment-Analysis)

（小声bb：如果这个项目对你有用，不妨给我一个免费的小星星，非常感谢！！！）
